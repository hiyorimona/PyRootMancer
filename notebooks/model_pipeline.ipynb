{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5402528667074840145\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5707399168\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9472595390804454493\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patchify import patchify, unpatchify\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, Softmax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = 'pipeline/processed/training/images/'\n",
    "# mask_folder = '../train/masks/_root/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_patchify(img_dir, save_dir, patch_size):\n",
    "    \"\"\"Adds padding to all images in folder and patchifies them using the patchify library. \n",
    "    The patchified images get saved in the specified folder and returned in an array.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Path to image folder \n",
    "        save_dir (str): Path to folder in which the patches should be saved\n",
    "        patch_size (int): Size of patches that should be made \n",
    "\n",
    "    Returns:\n",
    "        array: Array of patched images\n",
    "    \"\"\"\n",
    "    img = []\n",
    "    tifs = []\n",
    "\n",
    "    for image_filename in os.listdir(img_dir):\n",
    "        if image_filename.endswith((\".png\", \".tif\")):\n",
    "            im = cv2.imread(os.path.join(img_dir, image_filename))\n",
    "            img_name, extension = os.path.splitext(image_filename)\n",
    "\n",
    "            h, w = im.shape[:2]\n",
    "            height_padding = ((h // patch_size) + 1) * patch_size - h\n",
    "            width_padding = ((w // patch_size) + 1) * patch_size - w\n",
    "\n",
    "            top_padding = height_padding // 2\n",
    "            bottom_padding = height_padding - top_padding\n",
    "\n",
    "            left_padding = width_padding // 2\n",
    "            right_padding = width_padding - left_padding\n",
    "\n",
    "            padded_image = cv2.copyMakeBorder(im, top_padding, bottom_padding, left_padding, right_padding, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "            channels = 3 if extension == \".png\" else 1\n",
    "            patches = patchify(padded_image, (patch_size, patch_size, channels), step=patch_size)\n",
    "            patches = patches.reshape(-1, patch_size, patch_size, channels)\n",
    "\n",
    "            for i, patch in enumerate(patches):\n",
    "                output_filename = f\"{img_name}{i}{extension}\"\n",
    "                cv2.imwrite(os.path.join(save_dir, output_filename), patch)\n",
    "\n",
    "                if extension == \".png\":\n",
    "                    img.append(patch)\n",
    "                elif extension == \".tif\":\n",
    "                    tifs.append(patch)\n",
    "\n",
    "    return img, tifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = TP / (Positives+K.epsilon())\n",
    "        return recall\n",
    "    \n",
    "    def precision_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = TP / (Pred_Positives+K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    def f(y_true, y_pred):\n",
    "        intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "        total = K.sum(K.square(y_true),[1,2,3]) + K.sum(K.square(y_pred),[1,2,3])\n",
    "        union = total - intersection\n",
    "        return (intersection + K.epsilon()) / (union + K.epsilon())\n",
    "    return K.mean(f(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from Task 7 \n",
    "def padder(image, patch_size):\n",
    "    \"\"\"\n",
    "    Adds padding to an image to make its dimensions divisible by a specified patch size.\n",
    "\n",
    "    This function calculates the amount of padding needed for both the height and width of an image so that its dimensions become divisible by the given patch size. The padding is applied evenly to both sides of each dimension (top and bottom for height, left and right for width). If the padding amount is odd, one extra pixel is added to the bottom or right side. The padding color is set to black (0, 0, 0).\n",
    "\n",
    "    Parameters:\n",
    "    - image (numpy.ndarray): The input image as a NumPy array. Expected shape is (height, width, channels).\n",
    "    - patch_size (int): The patch size to which the image dimensions should be divisible. It's applied to both height and width.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The padded image as a NumPy array with the same number of channels as the input. Its dimensions are adjusted to be divisible by the specified patch size.\n",
    "\n",
    "    Example:\n",
    "    - padded_image = padder(cv2.imread('example.jpg'), 128)\n",
    "\n",
    "    \"\"\"\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    height_padding = ((h // patch_size) + 1) * patch_size - h\n",
    "    width_padding = ((w // patch_size) + 1) * patch_size - w\n",
    "\n",
    "    top_padding = int(height_padding/2)\n",
    "    bottom_padding = height_padding - top_padding\n",
    "\n",
    "    left_padding = int(width_padding/2)\n",
    "    right_padding = width_padding - left_padding\n",
    "\n",
    "    padded_image = cv2.copyMakeBorder(image, top_padding, bottom_padding, left_padding, right_padding, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net model copied from Task 7 Image segmentation with DL \n",
    "\n",
    "def unet_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    s = inputs\n",
    "\n",
    "    # Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(patch_size, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    # Expansive path \n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "     \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "     \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "     \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "     \n",
    "    # Adjust the number of output channels to match the number of classes\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_folders = ['_root','_shoot','_occluded_root','_seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7648 images belonging to 1 classes.\n",
      "Found 1911 images belonging to 1 classes.\n",
      "Found 7648 images belonging to 1 classes.\n",
      "Found 1911 images belonging to 1 classes.\n",
      "_root\n",
      "Epoch 1/100\n",
      "478/478 [==============================] - 86s 151ms/step - loss: 0.0435 - accuracy: 0.9876 - f1: 0.1472 - iou: 0.0367 - val_loss: 0.0398 - val_accuracy: 0.9908 - val_f1: 0.3945 - val_iou: 0.0916\n",
      "Epoch 2/100\n",
      "478/478 [==============================] - 73s 153ms/step - loss: 0.0152 - accuracy: 0.9953 - f1: 0.5990 - iou: 0.1267 - val_loss: 0.0203 - val_accuracy: 0.9931 - val_f1: 0.5973 - val_iou: 0.1295\n",
      "Epoch 3/100\n",
      "478/478 [==============================] - 73s 153ms/step - loss: 0.0109 - accuracy: 0.9962 - f1: 0.6848 - iou: 0.1504 - val_loss: 0.0182 - val_accuracy: 0.9935 - val_f1: 0.6190 - val_iou: 0.1432\n",
      "Epoch 4/100\n",
      "478/478 [==============================] - 74s 155ms/step - loss: 0.0089 - accuracy: 0.9967 - f1: 0.7466 - iou: 0.1617 - val_loss: 0.0141 - val_accuracy: 0.9945 - val_f1: 0.7071 - val_iou: 0.1573\n",
      "Epoch 5/100\n",
      "478/478 [==============================] - 73s 152ms/step - loss: 0.0078 - accuracy: 0.9970 - f1: 0.7765 - iou: 0.1693 - val_loss: 0.0132 - val_accuracy: 0.9947 - val_f1: 0.7540 - val_iou: 0.1811\n",
      "Epoch 6/100\n",
      "478/478 [==============================] - 73s 153ms/step - loss: 0.0076 - accuracy: 0.9971 - f1: 0.7668 - iou: 0.1706 - val_loss: 0.0120 - val_accuracy: 0.9951 - val_f1: 0.7711 - val_iou: 0.1770\n",
      "Epoch 7/100\n",
      "478/478 [==============================] - 73s 153ms/step - loss: 0.0073 - accuracy: 0.9972 - f1: 0.7828 - iou: 0.1743 - val_loss: 0.0131 - val_accuracy: 0.9947 - val_f1: 0.7326 - val_iou: 0.1637\n",
      "Epoch 8/100\n",
      "478/478 [==============================] - 74s 154ms/step - loss: 0.0072 - accuracy: 0.9972 - f1: 0.7835 - iou: 0.1752 - val_loss: 0.0131 - val_accuracy: 0.9948 - val_f1: 0.7616 - val_iou: 0.1774\n",
      "Epoch 9/100\n",
      "478/478 [==============================] - 73s 153ms/step - loss: 0.0070 - accuracy: 0.9973 - f1: 0.7918 - iou: 0.1760 - val_loss: 0.0117 - val_accuracy: 0.9951 - val_f1: 0.7732 - val_iou: 0.1891\n",
      "Epoch 10/100\n",
      "478/478 [==============================] - 74s 154ms/step - loss: 0.0067 - accuracy: 0.9973 - f1: 0.7957 - iou: 0.1792 - val_loss: 0.0120 - val_accuracy: 0.9951 - val_f1: 0.7735 - val_iou: 0.1792\n",
      "Epoch 11/100\n",
      "478/478 [==============================] - 74s 154ms/step - loss: 0.0067 - accuracy: 0.9974 - f1: 0.7999 - iou: 0.1799 - val_loss: 0.0115 - val_accuracy: 0.9953 - val_f1: 0.7696 - val_iou: 0.1855\n",
      "Epoch 12/100\n",
      "239/478 [==============>...............] - ETA: 33s - loss: 0.0068 - accuracy: 0.9974 - f1: 0.7839 - iou: 0.1692"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Save the best model with a unique name for each folder\u001b[39;00m\n\u001b[0;32m     79\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m), save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 81\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_image_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m validation_image_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m validation_image_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m     91\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Jakub\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Jakub\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Jakub\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Jakub\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jakub\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Jakub\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Jakub\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for mask_folder in mask_folders:\n",
    "    img_folder = f'raw/train/images/'\n",
    "    masks_folder = f'raw/train/masks/{mask_folder}/'\n",
    "    patch_size = 256\n",
    "\n",
    "    \n",
    "    # Training images\n",
    "    train_image_datagen = ImageDataGenerator(validation_split=0.2,rescale=1./255)\n",
    "    train_image_generator = train_image_datagen.flow_from_directory(\n",
    "        img_folder,\n",
    "        target_size=(patch_size, patch_size),\n",
    "        batch_size=16,\n",
    "        class_mode=None,  # None since you don't want labels for images\n",
    "        color_mode='rgb',\n",
    "        seed=42,\n",
    "        subset='training'  # specify the subset as 'training'\n",
    "    )\n",
    "\n",
    "    # Validation images\n",
    "    validation_image_generator = train_image_datagen.flow_from_directory(\n",
    "        img_folder,\n",
    "        target_size=(patch_size, patch_size),\n",
    "        batch_size=16,\n",
    "        class_mode=None,  # None since you don't want labels for images\n",
    "        color_mode='rgb',\n",
    "        seed=42,\n",
    "        subset='validation'  # specify the subset as 'validation'\n",
    "    )\n",
    "\n",
    "    # Training masks\n",
    "    train_mask_datagen = ImageDataGenerator(validation_split=0.2)  # You can also use validation_split here\n",
    "    train_mask_generator = train_mask_datagen.flow_from_directory(\n",
    "        masks_folder,\n",
    "        target_size=(patch_size, patch_size),\n",
    "        batch_size=16,\n",
    "        color_mode='grayscale',  # Grayscale for multiclass segmentation\n",
    "        class_mode=None,\n",
    "        seed=42,\n",
    "        subset='training'  # specify the subset as 'training'\n",
    "    )\n",
    "\n",
    "    # Validation masks\n",
    "    validation_mask_generator = train_mask_datagen.flow_from_directory(\n",
    "        masks_folder,\n",
    "        target_size=(patch_size, patch_size),\n",
    "        batch_size=16,\n",
    "        color_mode='grayscale',  # Grayscale for multiclass segmentation\n",
    "        class_mode=None,\n",
    "        seed=42,\n",
    "        subset='validation'  # specify the subset as 'validation'\n",
    "    )\n",
    "    validation_image_generator.samples//16\n",
    "    len(train_image_generator)\n",
    "    print(mask_folder)\n",
    "    train_generator = zip(train_image_generator, train_mask_generator)\n",
    "    validation_generator = zip(validation_image_generator, validation_mask_generator)\n",
    "\n",
    "    # Define the U-Net model\n",
    "    input_shape = (256, 256, 3)\n",
    "    num_classes = 1\n",
    "    model = unet_model(input_shape, num_classes)\n",
    "    optimizer=Adam()\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', f1, iou])\n",
    "\n",
    "    # # Print a summary of the model architecture\n",
    "    # model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, mode='min')\n",
    "\n",
    "\n",
    "    # Define the directory to save the best models\n",
    "    model_save_dir = '../best_models/'\n",
    "\n",
    "\n",
    "    # Save the best model with a unique name for each folder\n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(model_save_dir, f'best_model{mask_folder}.h5'), save_best_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_image_generator),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_image_generator.samples // 16,\n",
    "        epochs=100,\n",
    "        callbacks=[early_stopping, model_checkpoint]\n",
    "    )\n",
    "    num_batches = validation_image_generator.samples // validation_image_generator.batch_size\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    \n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        img_batch, mask_batch = next(validation_generator)\n",
    "        preds = model.predict(img_batch)\n",
    "        all_predictions.append(preds)\n",
    "        all_labels.append(mask_batch)\n",
    "\n",
    "    # Concatenate predictions and labels across batches\n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate IoU for the entire validation set\n",
    "    iou_value = iou(K.variable(labels), K.variable(predictions))\n",
    "    iou_value = K.eval(iou_value)\n",
    "    print(\"IoU:\", iou_value)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
